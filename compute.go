package compute

import (
   lmdb "github.com/szferi/gomdb"
)

type RequestCmd int 

const (
   Get  RequestCmd = iota
   Put
   Find
   Update
   Delete
)


/*

 The compute code processes a very simple bytecode language. The language provides codes for performing very
 specific operations on data in order to find matching rows. The compute server only processes predicates,
 it does not attempt to perform extractions. Instead, it just keeps track of rows that match.

 Predicate matching works like this:

 1. For each expression a bit is reserved.
 2. For each term a bit pattern is generated.
 3. During evaluation of each row, bits are set to true for each expression evaluating to true.
 4. The generated bit patterns are compared against the bits set by evaluation. As soon as any pattern
    matches we select the row. If we evaluate all expressions without a bit pattern match then the row
    does not match the selection criteria.

 In theory this lets us evaluate each expression once, even though the expression may be present multiple times
 in the query.

 For example:

 SELECT * FROM test WHERE (c1 > 5 AND c2='train') OR (c1 < 2 AND c2='train');

 Granted, this expression could be rewritten more efficiently, but it servers to illustrate a point. Expressions
 cannot always be rewritten in a simple way, or the expressions may be generated by code and the user may never
 have an opportunity to optimize the query.

 In this case we have two terms:
   1. (c1>5 AND c2='train')
   2. (c1<2 AND c2='train')

 There are only three expressions:
   1. c1>5
   2. c1<2
   3. c2='train'

 In this case we might assign the bit indexes to 0, 1, and 2 respectively. So the bit patterns we expect to see
 for each term are:
   1. 101
   2. 110

 If we evaluate the expressions against (c1=9, c2='train'), (c1=1, c2='train'), and (c1=1, c2='plane'):
   1. 101
   2. 110
   3. 010


 What about:

 SELECT * FROM test WHERE (c1 < 2 AND c2='train') OR (c2='plane');

 The bit patterns should be:
   1. 011
   2. 100

 Evaluated against our previous example row:
   1. 001
   2. 011
   3. 101

 Now, this is interesting because we got an additional bit we weren't expecting in the second term. We can't
 perform an exact match because of the additional bits. We don't want to reset them in every situation though,
 because that undermines the entire purpose of the feature. Instead, we just need to mask the result with
 the bits that we care about. Which also happens to be the bit pattern for the term.

 So the compare operation becomes something more like:

 match := (evaluation & pattern) == pattern;

 Using the masked version of the data hides the 'spurious' result and allows us to perform a perfect match.

 An additional optimization enabled by this method involves efficient processing of each column. For example,
 we could do the two tests for c1 (in the first example) while we have the c1 value for the row. That avoids
 deserializing the value twice, or having to cache it in a deserialized format somewhere.

 */

type Opcode byte

const (
   Nop   Opcode = iota
   Eq
   Ne
   Ge
   Le
   Gt
   Lt
)

type Literal struct {
   Column int
   Value []byte
}

type Subselect struct {
   Column int
   ResultSet int
}

type Op struct {
   Cmd Opcode
   ResultBit byte
   Target interface{}
}

type Request struct {
   Cmd RequestCmd
   Predicate []Op
}


func compute_server(c chan) {
}

func Start() channel {
   c := make(chan *Request, 100);
   go compute_server(c);
}



